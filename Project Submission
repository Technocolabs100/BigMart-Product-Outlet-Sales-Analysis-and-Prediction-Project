import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from joblib import dump

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

print("Train Data:")
print(train_data.head())
print("Test Data:")
print(test_data.head())
print(train_data.describe())


plt.figure(figsize=(12, 6))
sns.histplot(data=train_data, x='Item_Outlet_Sales', bins=30)
plt.title('Distribution of Item Outlet Sales')
plt.show()
plt.figure(figsize=(12, 6))
sns.boxplot(data=train_data, x='Outlet_Location_Type', y='Item_Outlet_Sales')
plt.title('Outlet Location Type vs Item Outlet Sales')
plt.show()
train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)
train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)
train_data['Item_Visibility_bins'] = pd.cut(train_data['Item_Visibility'], [0, 0.065, 0.13, 0.2], labels=['Low Viz', 'Viz', 'High Viz'])
train_data['Outlet_Years'] = 2023 - train_data['Outlet_Establishment_Year']
label_encoder = LabelEncoder()
train_data['Item_Fat_Content'] = label_encoder.fit_transform(train_data['Item_Fat_Content'])
test_data['Item_Fat_Content'] = label_encoder.transform(test_data['Item_Fat_Content'])
categorical_columns = ['Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
combined_data = pd.concat([train_data, test_data], ignore_index=True)
combined_data_encoded = pd.get_dummies(combined_data, columns=categorical_columns)

train_data_encoded = combined_data_encoded.iloc[:train_data.shape[0]]
test_data_encoded = combined_data_encoded.iloc[train_data.shape[0]:]

features_scaled = train_data_encoded.drop('Item_Outlet_Sales', axis=1)
target = train_data_encoded['Item_Outlet_Sales']

categorical_columns = features_scaled.select_dtypes(include=[np.object]).columns
features_encoded = pd.get_dummies(features_scaled, columns=categorical_columns)
features_encoded = features_encoded[train_data_encoded.columns[:-1]]
train_features = features_encoded[:len(train_data)]
test_features = features_encoded[len(train_data):]

linear_reg = LinearRegression()
linear_reg.fit(train_features, target)
linear_reg_predictions = linear_reg.predict(train_features)
linear_reg_rmse = mean_squared_error(target, linear_reg_predictions, squared=False)
print("Linear Regression RMSE:", linear_reg_rmse)

ridge_reg = Ridge()
ridge_reg.fit(train_features, target)
ridge_reg_predictions = ridge_reg.predict(train_features)
ridge_reg_rmse = mean_squared_error(target, ridge_reg_predictions, squared=False)
print("Ridge Regression RMSE:", ridge_reg_rmse)

rf_reg = RandomForestRegressor(random_state=42)
rf_reg.fit(train_features, target)
rf_reg_predictions = rf_reg.predict(train_features)
rf_reg_rmse = mean_squared_error(target, rf_reg_predictions, squared=False)
print("Random Forest Regression RMSE:", rf_reg_rmse)

xgb_reg = XGBRegressor(random_state=42)
xgb_reg.fit(train_features, target)
xgb_reg_predictions = xgb_reg.predict(train_features)
xgb_reg_rmse = mean_squared_error(target, xgb_reg_predictions, squared=False)
print("XGBoost Regression RMSE:", xgb_reg_rmse)

ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}
ridge_grid = GridSearchCV(ridge_reg, ridge_params, scoring='neg_root_mean_squared_error', cv=5)
ridge_grid.fit(features_scaled, target)
best_ridge_rmse = -ridge_grid.best_score_
print("Best Ridge Regression RMSE:", best_ridge_rmse)

rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}
rf_grid = GridSearchCV(rf_reg, rf_params, scoring='neg_root_mean_squared_error', cv=5)
rf_grid.fit(features_scaled, target)
best_rf_rmse = -rf_grid.best_score_
print("Best Random Forest Regression RMSE:", best_rf_rmse)

xgb_params = {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7]}
xgb_grid = GridSearchCV(xgb_reg, xgb_params, scoring='neg_root_mean_squared_error', cv=5)
xgb_grid.fit(features_scaled, target)
best_xgb_rmse = -xgb_grid.best_score_
print("Best XGBoost Regression RMSE:", best_xgb_rmse)

best_model = xgb_grid.best_estimator_
dump(best_model, 'best_model.joblib')


