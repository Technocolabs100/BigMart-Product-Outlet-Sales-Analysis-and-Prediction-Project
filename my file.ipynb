Here is the link of my Notebook in case needed: https://colab.research.google.com/drive/1PnlIAD3T2VtN4Cwnkhh-B6bTNXptiWhr?usp=sharing
Technocolabs Mini Project: Bigmart Sales Dataset

Introduction:

Problem Statement: The data scientists at BigMart have collected sales data for 1559 products across 10 stores in different cities for the year 2013. Now each product has certain attributes that sets it apart from other products.

Breakdown of the Problem Statement: Supervised machine learning problem. The target value will be Item_Outlet_Sales.

Data Id ðŸ“‹ This dataset is named [BigMart Sales]. The dataset contains a set of 1559 under 10 stores:

Column Name Description Item_Identifier Unique product ID Item_Weight Weight of product Item_Fat_Content Checks the Concentration of fat in the product Item_Visibility The % of total display area of all similar products in a store Item_Type Product Category Item_MRP Maximum Retail Price for a Product Outlet_Identifier Store ID Outlet_Establishment_Year The year in which store was established Outlet_Size The size of the store (Area Size Category) Outlet_Location_Type In Terms of city Tiers (Size) Outlet_Type Grocery store or a type of supermarket Item_Outlet_Sales Sales of the product In the Specific outlet

Libraries ðŸ“•ðŸ“—ðŸ“˜

import pandas as pd
import numpy as nm
#ploting libraries
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor


#feature engineering
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler
#train test split
from sklearn.model_selection import train_test_split, GridSearchCV
#metrics
from sklearn.metrics import mean_squared_error

from joblib import dump
from xgboost import XGBRegressor

First look at the data: Here, we load the training and testing datasets into pandas dataframes
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

Data Structure and Content
print("Train Data:")
print(train_data.head())
print("Test Data:")
print(test_data.head())

Exploratory Data Analysis
print(train_data.describe())

Univariate Analysis; a histogram to visualize the distribution of the target variable, "Item_Outlet_Sales," in the training dataset.
plt.figure(figsize=(12, 6))
sns.histplot(data=train_data, x='Item_Outlet_Sales', bins=30)
plt.title('Distribution of Item Outlet Sales')
plt.show()

Bivariate Analysis:

a boxplot to explore the relationship between the "Outlet_Location_Type" and "Item_Outlet_Sales" variables in the training dataset
plt.figure(figsize=(12, 6))
sns.boxplot(data=train_data, x='Outlet_Location_Type', y='Item_Outlet_Sales')
plt.title('Outlet Location Type vs Item Outlet Sales')
plt.show()

Missing Value Treatment: filling the missing values in the "Item_Weight" column with the mean value and filling the missing values in the "Outlet_Size" column with the mode value
train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)
train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)

Feature Engineering: perform feature engineering by creating a new categorical feature, "Item_Visibility_bins," based on the "Item_Visibility" column. We also calculate the "Outlet_Years" by subtracting the "Outlet_Establishment_Year" from 2023. Finally, we assign the target variable, "Item_Outlet_Sales," to the variable "target.
train_data['Item_Visibility_bins'] = pd.cut(train_data['Item_Visibility'], [0, 0.065, 0.13, 0.2], labels=['Low Viz', 'Viz', 'High Viz'])
train_data['Outlet_Years'] = 2023 - train_data['Outlet_Establishment_Year']
target = train_data['Item_Outlet_Sales']

Encoding Categorical Variables: label encoding to convert the categorical variable "Item_Fat_Content" into numerical representation in both the training and testing datasets.
label_encoder = LabelEncoder()
train_data['Item_Fat_Content'] = label_encoder.fit_transform(train_data['Item_Fat_Content'])
test_data['Item_Fat_Content'] = label_encoder.transform(test_data['Item_Fat_Content'])

One-Hot Encoding: one-hot encoding on the categorical columns in the training dataset, creating binary columns for each category. The resulting encoded columns are added to the dataframe. We assign the numerical columns (excluding the target and categorical columns) to the "features" variable.
categorical_columns = ['Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']
train_data = pd.get_dummies(train_data, columns=categorical_columns)
features = train_data[numerical_columns]

Preprocessing Data: separate the numerical columns from the "features" dataframe. We then apply standard scaling to the numerical features to normalize their values. Finally, we concatenate the scaled numerical features with the one-hot encoded categorical columns to create the final processed dataset, "features_scaled".
numerical_columns = [col for col in features.columns if col not in categorical_columns]
numerical_features = features[numerical_columns]
scaler = StandardScaler()
numerical_features_scaled = scaler.fit_transform(numerical_features)
features_scaled = np.concatenate((numerical_features_scaled, train_data[categorical_columns]), axis=1)

Modeling: initialize and train a linear regression model using the processed features ("features_scaled") and the target variable ("target")
linear_reg = LinearRegression()
linear_reg.fit(features_scaled, target)

Regularized Linear Regression: initialize a ridge regression model and perform hyperparameter tuning using grid search cross-validation. The best estimator is selected based on the negative mean squared error (scoring metric).
ridge_reg = Ridge()
ridge_params = {'alpha': [0.001, 0.01, 0.1, 1, 10]}
ridge_grid = GridSearchCV(ridge_reg, ridge_params, scoring='neg_mean_squared_error', cv=5)
ridge_grid.fit(features_scaled, target)
ridge_best = ridge_grid.best_estimator_

Random Forest; initialize a random forest regressor and perform hyperparameter tuning using grid search cross-validation. The best estimator is selected based on the negative mean squared error (scoring metric)
rf_reg = RandomForestRegressor()
rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [5, 8, 10], 'min_samples_split': [2, 5, 10]}
rf_grid = GridSearchCV(rf_reg, rf_params, scoring='neg_mean_squared_error', cv=5)
rf_grid.fit(features_scaled, target)
rf_best = rf_grid.best_estimator_

XGBoost: initialize an XGBoost regressor and perform hyperparameter tuning using grid search cross-validation. The best estimator is selected based on the negative mean squared error (scoring metric).
xgb_reg = XGBRegressor()
xgb_params = {'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200, 300]}
xgb_grid = GridSearchCV(xgb_reg, xgb_params, scoring='neg_mean_squared_error', cv=5)
xgb_grid.fit(features_scaled, target)
xgb_best = xgb_grid.best_estimator_

Saving the Final Model: save the best-performing model (in this case, the random forest regressor) as a joblib file named "final_model.joblib" for future use.
dump(rf_best, 'final_model.joblib')

Final Predictions on the Test Dataset: preprocess the test dataset by handling missing values, label encoding categorical variables, applying the same feature transformations as done on the training dataset, and performing one-hot encoding. Then, we make predictions using the saved models (linear regression, ridge regression, random forest, and XGBoost) on the processed test dataset.
# Preprocessing test dataset
test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)
test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)
test_data['Item_Fat_Content'] = label_encoder.transform(test_data['Item_Fat_Content'])
test_data['Item_Visibility_bins'] = pd.cut(test_data['Item_Visibility'], [0, 0.065, 0.13, 0.2], labels=['Low Viz', 'Viz', 'High Viz'])
test_data = pd.get_dummies(test_data, columns=categorical_columns)
test_features = test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1)
test_features_scaled = scaler.transform(test_features)

# Making predictions
linear_reg_predictions = linear_reg.predict(test_features_scaled)
ridge_predictions = ridge_best.predict(test_features_scaled)
rf_predictions = rf_best.predict(test_features_scaled)
xgb_predictions = xgb_best.predict(test_features_scaled)
